cluster-delegator Documentation
===============================
Shaun Harker <sharker@math.rutgers.edu>
v2.0, 2015-07-03
:toc: right
:stem: latexmath
:source-highlighter: highlight.js
:highlightjs-theme: railscasts


== Overview

Cluster-Delegator is a developer package to assist in writing C++ applications utilizing MPI on a cluster in a single-coordinator/many-worker (star topology) arrangement.

=== Acknowledgements

This software was developed in response to group discussions involving Shaun Harker, Pawel Pilarczyk, and Ippei Obayashi in 2010. 

The Coordinator-Worker scheme is described in Section 3 of the following paper:

----
 P. Pilarczyk, Parallelization method for a continuous property,
 Foundations of Computational Mathematics, Vol. 10, No. 1 (2010),
 93-114. DOI: 10.1007/s10208-009-9050-8.
----

== Installation 

=== Dependencies

* Boost
* OpenMPI

=== Instructions

There are two ways to install. One is to use the `./install.sh` script as follows:

```bash
./install --prefix=/path/to/install/folder
```

whereupon the headers (this is a header only library) will be copied into
```bash
/path/to/install/include/delegator
```

Another way to install works on Mac OS X with http://brew.sh[homebrew] installed:

```bash
brew shaunharker/tap/cluster-delegator
```

Homebrew of course installs into 

```bash
/usr/local
```


== Examples 

=== Skeleton Example


The simplest program using the software is as follows:


```cpp
#include "delegator/delegator.h"

class Process : public Coordinator_Worker_Process {};

int main ( int argc, char * argv [] ) {
  delegator::Start ();
  delegator::Run < Process > (argc, argv); 
  delegator::Stop ();
}
```

This program creates no jobs, sends no jobs, works no jobs, creates no results, and stores no results. But it does initialize the delegator system. And it can easily be modified into a program
that DOES do something, by fleshing out class Process by providing overrides of methods in Coordinator_Worker_Process. These methods are discussed below.


=== Included Examples

The quickest way (for the brave) to learn how to use cluster-delegator is to look at the examples 
```
   ./examples/example1.cpp 
   ./examples/example2.cpp
```

You should take a look at these and try to reason them out a little bit. Many people will probably be quite close to understanding more or less how the software works just by seeing these examples. (But don't worry, we give an explanation below if it doesn't just click!)




== The Scheme 


In order to actually make something happen, the user needs to
override methods from `Coordinator_Worker_Process`. To see how
this works, we'll describe the behavior of the algorithm called by
`Run<Process>()`:


* Of the N processes, N-1 are workers and 1 is a coordinator.

* The coordinator is responsible for writing "jobs" and
reading "results". 

* The workers are responsible for accepting "jobs" and working them,
thus producing "results" 

In step-by-step detail:


. One of the processes decides it is a coordinator, the others decide they are workers. The processes each execute the method
+
```cpp
void Process::command_line ( int argc, char * argv [] );
```
+
to store/process the command line arguments.  
+
. The coordinator process runs (one time only) the method
+
```cpp
void Process::initialize ( void );
```    
+
. The next three things happen in an interspersed fashion, on demand as workers, jobs, and results become available:
+
.. The coordinator process calls the method 
+
```cpp
int Process::prepare ( Message & job_message )
```
+
in order to create "job_message" which is sent to a worker.
+
.. A worker process calls the method 
+
```cpp
void Process::work ( Message & result_message, 
                    Message const& job_message ) const;
```
+
on "job_message" received from the coordinator in order to produce "result_message" which is sent back to the coordinator.
+
.. The coordinator process calls the method
+
```cpp
void Process::accept ( Message const& result_message )
```
+
on the "result_message" which was received from a worker.
+
. The coordinator process runs (one time only) the method
```cpp
void Process::finalize ( void );
```

Note that routines in step 3 will happen many times and be interspersed,
but the routines in 1, 2, and 4 each happen only once and happen in the order 
they are listed.

== Jobs, Results, and Messages

Jobs and Results in cluster-delegator are the messages passed between `prepare`, `work`, and `accept` methods. They are objects of type `class Message`. The `Message` class provides a uniform method to "serialize" your data so it can be passed around by the processes. In fact, `class Message` is really just a wrapper around the Boost serialization package.

The semantics of `class Message` are as follows:

```cpp
Message job_message;  // now I have an empty message
int jobdata1; 
char jobdata2; 
std::vector < int > jobdata3; 
std::unordered_map < int, float > jobdata4;
std::string jobdata5;
// code to produce data in jobdata variables
job_message << jobdata1;
job_message << jobdata2;
job_message << jobdata3;
job_message << jobdata4;
```


Later, this data may be extracted in precisely the same way, except we replace the << operators with >> operators. And you must extract the data in the same order it was inserted! (Not the reverse order, for example, or in a random order.)


== Methods


There are six methods which may be overridden in
Coordinator_Worker_Process. They are:


. `command_line`: Set command line arguments so they are available to `Process`
. `initialize`: Initialize in some fashion for what is to come
. `prepare`: Come up with a job to send to a worker and return 0, OR realize there are no jobs left to send and return 1, OR stall for results before producing a job and return 2
. `work`: work a job and produce a result
. `accept`: accept a result and handle it in some way
. `finalize`: Finish up after all jobs have been `accept`ed do whatever is left to do before the program shuts down.

Each of these methods has a default behavior, specified in the base class Coordinator_Worker_Process. These functions are called by the Coordinator_Worker_Scheme algorithm. Since a process is either a coordinator or a worker, the methods divide up into the methods called by the coordinator process, and the methods called by the worker processes, and methods called by both.


* All processes: `command_line`
* Coordinator only: `initialize`, `prepare`, `accept`, `finalize`
* Worker only: `work`


Only one of these processes has a return value, which is `prepare`. It returns 0, 1, or 2, depending on the situation. For simple programs, it returns 0 while it is producing jobs, and then switches to returning 1 to indicate there are no jobs left. More sophisticated programs might make use of the 2 option. The 2 options says "There will be more jobs, but not right now." This would be used in the situation where results need to be `accept`ed first to determine what ought to be computed next.


== Member Variables

`Coordinator_Worker_Process` has several member variables, which are inherited by the user-supplied `Process`, and are used by default implementations of the methods.


```cpp
/*----- Command line arguments -----*/
int argc;
char * * argv;
```

The default behavior of `command_line` is to set `argc` and `argv` to match the command line parameters handed to main.


```cpp
/*----- The Message Stacks ------*/
std::stack < Message > JOBS_TO_SEND; 
std::stack < Message > RECEIVED_RESULTS;
```


`JOBS_TO_SEND` is a stack which is intended to be populated by the user override of "initialize". The user would create jobs, make them into job messages, and push them onto the `JOBS_TO_SEND` stack. The default implementation of the `prepare` method pops a job message from `JOBS_TO_SEND` and writes to its argument (which is then sent off to a worker.)

NOTE: It is *not* necessary to use `JOBS_TO_SEND`. This is for convenience only along with usage of the default `prepare` method.


`RECEIVED_RESULTS` is a stack which is automatically populated by the default implementation of `accept`. The default implementation of `accept` takes its argument (which is a result message received from some worker) and pushes it onto the `RECEIVED_RESULTS` stack. The user can then write an override for finalize in order to go through the `RECEIVED_RESULTS` stack and handle the results however they wish.

NOTE: It is *not* necessary to use `RECEIVED_RESULTS`. This is for convenience only along with usage of the default `accept` method.

== Default Methods 

Here we describe the default behaviors of the methods of `Coordinator_Worker_Process`. These behaviors have been discussed already with respect to how they affect the member variables (in particular the message stacks), but we error on the side of redundancy and give them again here.


* `command_line:`: Sets member variables argc and argv to match the passed parameters

* `initialize`: Does nothing by default

* `prepare`: Checks if there is an item on member variable
```cpp
std::stack < Message > JOBS_TO_SEND
```
if so, it pops that item from the stack and writes it to 
`job_message` and returns code 0, which means "i produced a job." Otherwise, it returns code 1, which means "no jobs left to produce" It NEVER returns code 2, which would mean "I am waiting on information before I can produce more jobs"

* `work`: Does nothing by default

* `accept`
 pushes the message "result_message" onto the member variable 
```cpp
std::stack < Message > RECEIVED_RESULTS
```
* `finalize`: Does nothing by default

== Examples in depth

=== `example1`

The easiest way to use the software is to only override
`initialize`, `work`, and `finalize`. This is what is done in
```bash
./examples/example1.cpp
```

Then we will have

* `initialize`: Invent jobs, turn them into `job_message`s, and push them onto 
the member variable
```cpp
std::stack < Message > JOBS_TO_SEND
```

* `finalize`: Read the result_messages off the member variable
```cpp
std::stack < Message > RECEIVED_RESULTS
```
and extract the results and deal with them as necessary.
* `work`: Extract the job from `job_message`, do a computation on the job to produce a result,
   create result_message from the result 


=== `example2`

One does not have to use the default behavior of `prepare` and `accept` 
and may override them not to use the `JOBS_TO_SEND` and `RECEIVED_RESULTS` 
stacks at all. 

By overriding all the methods, one has a considerable degree of control, and can even arrange to wait until certain results have arrived before determining what the next jobs should be. The function `prepare` can report a return value of 2 if it wishes to indicate it is in this "waiting for more results to be dealt with by `accept` before I can create a new job". This allows `prepare` to not produce a job_message, but not cause the distributed computing scheme to believe there are no jobs left and start retiring free workers.

== Building MPI Programs


To compile the examples, type (at the root of the distribution)
```bash
cd examples
make clean
make
```

Or, to do it by hand:

```bash
cd examples
```

Compile with:

```bash
mpicxx -O3 -I../include/ -c -o example1.o example1.cpp
mpicxx -O3 -I../include/ -c -o example2.o example2.cpp
```

Link with:

```bash
mpicxx -lboost_serialization example1.o -o example1
mpicxx -lboost_serialization example2.o -o example2
```

== Running MPI Programs

=== Single Machine

To run the first example, type
```bash
 mpiexec -np 8 ./example1
```

To run the second example, type
```bash
 mpiexec -np 8 ./example2 42 is the answer
```

The number "8" is arbitrary; it's the number of processes you'd like. If you 
have a double core system, probably "3" is a good choice (one of the processes
will spend most of its time sleeping.) It's OK to put more cores than you have -- the OS can schedule them.


=== Cluster


Similar commands to the previous will probably work for clusters, though
you probably need more command line options (e.g. hostfiles and such). But likely, you will probably be required by system administrators to use scheduling software so you can share the system. Unless you just happen to own your very own super-duper cluster. 

There are three scheduling programs we will discuss: PBS, SGE, and SLURM.

==== PBS

This is accomplished by writing a PBS script. 

Here is what such a script looks like:

```bash
#!/bin/bash
#PBS -l nodes=10:ppn=8
cd $PBS_O_WORKDIR
mpiexec ./my_program
```

In this simple example, we specified to use 10 nodes with 8 processors per
node.



To submit the program to the cluster, one would type
```bash
ssh my_account@my.fancy.cluster.edu
# ... get it ready ...
qsub script.sh   #submit the job!
```



You can periodically check the progress of your computation by typing
```bash
qstat
```



If something seems wrong, you can terminate your program with
```bash
qdel 
```
followed by your job number (which you can see from qstat)


==== SGE

TODO

==== SLURM

TODO


== History

* 2015-07-03. Releasing v2.0. 
** Updated documentation
** Added support for return value of 2 from `prepare` (i.e. "no jobs for now, but maybe later")
** Switched to thread-based approach to handle communicator
** Removed 128MB size limitation on messages
** Introduced auto-sizing buffer

* 2014-08-13 -- migrated to github

* 2011-08-24 -- hosted on googlecode

* Initial development was done in 2010. 

== Support

See the included examples, and you should be able to sort it out! If not, email me at sharker81@gmail.com.


